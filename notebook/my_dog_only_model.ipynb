{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"C:/Users/dedsec995/Projects/tensorflow/breed_dog_only/Data/dog/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((224, 224, 3))\n",
    "backbone = DenseNet121(input_tensor=inp,\n",
    "                       weights=\"imagenet\",\n",
    "                       include_top=False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outp = Dense(120, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inp, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-6]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18571 images belonging to 120 classes.\n",
      "Found 2009 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                   rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   validation_split=0.1\n",
    ")\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size  = 64,\n",
    "                                                    color_mode = 'rgb',\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size = (224, 224),\n",
    "                                                    subset='training')     \n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator =  train_datagen.flow_from_directory( train_dir,\n",
    "                                                          batch_size  = 64, \n",
    "                                                          color_mode = 'rgb',\n",
    "                                                          class_mode=\"categorical\",\n",
    "                                                          target_size = (224, 224),\n",
    "                                                          subset='validation')\n",
    "\n",
    "def my_gen(gen):\n",
    "    while True:\n",
    "        try:\n",
    "            data, labels = next(gen)\n",
    "            yield data, labels\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "291/291 [==============================] - 231s 794ms/step - loss: 3.0085 - accuracy: 0.2725 - val_loss: 1.5863 - val_accuracy: 0.5734\n",
      "Epoch 2/60\n",
      "291/291 [==============================] - 234s 804ms/step - loss: 1.8345 - accuracy: 0.4864 - val_loss: 1.3661 - val_accuracy: 0.6167\n",
      "Epoch 3/60\n",
      "291/291 [==============================] - 231s 795ms/step - loss: 1.6300 - accuracy: 0.5351 - val_loss: 1.3095 - val_accuracy: 0.6212\n",
      "Epoch 4/60\n",
      "291/291 [==============================] - 235s 806ms/step - loss: 1.5271 - accuracy: 0.5579 - val_loss: 1.2659 - val_accuracy: 0.6297\n",
      "Epoch 5/60\n",
      "291/291 [==============================] - 230s 790ms/step - loss: 1.4444 - accuracy: 0.5849 - val_loss: 1.2643 - val_accuracy: 0.6322\n",
      "Epoch 6/60\n",
      "291/291 [==============================] - 224s 771ms/step - loss: 1.4323 - accuracy: 0.5834 - val_loss: 1.2379 - val_accuracy: 0.6317\n",
      "Epoch 7/60\n",
      "291/291 [==============================] - 224s 771ms/step - loss: 1.3863 - accuracy: 0.5950 - val_loss: 1.2119 - val_accuracy: 0.6411\n",
      "Epoch 8/60\n",
      "291/291 [==============================] - 211s 725ms/step - loss: 1.3663 - accuracy: 0.6042 - val_loss: 1.2170 - val_accuracy: 0.6297\n",
      "Epoch 9/60\n",
      "291/291 [==============================] - 208s 715ms/step - loss: 1.3210 - accuracy: 0.6111 - val_loss: 1.2055 - val_accuracy: 0.6521\n",
      "Epoch 10/60\n",
      "291/291 [==============================] - 215s 741ms/step - loss: 1.3111 - accuracy: 0.6121 - val_loss: 1.1895 - val_accuracy: 0.6481\n",
      "Epoch 11/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.2859 - accuracy: 0.6252 - val_loss: 1.1946 - val_accuracy: 0.6585\n",
      "Epoch 12/60\n",
      "291/291 [==============================] - 214s 736ms/step - loss: 1.2698 - accuracy: 0.6235 - val_loss: 1.1778 - val_accuracy: 0.6411\n",
      "Epoch 13/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.2559 - accuracy: 0.6259 - val_loss: 1.1891 - val_accuracy: 0.6456\n",
      "Epoch 14/60\n",
      "291/291 [==============================] - 212s 729ms/step - loss: 1.2419 - accuracy: 0.6326 - val_loss: 1.1869 - val_accuracy: 0.6446\n",
      "Epoch 15/60\n",
      "291/291 [==============================] - 212s 729ms/step - loss: 1.2204 - accuracy: 0.6377 - val_loss: 1.1484 - val_accuracy: 0.6560\n",
      "Epoch 16/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.2157 - accuracy: 0.6379 - val_loss: 1.1598 - val_accuracy: 0.6675\n",
      "Epoch 17/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.2092 - accuracy: 0.6409 - val_loss: 1.1862 - val_accuracy: 0.6536\n",
      "Epoch 18/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.1886 - accuracy: 0.6454 - val_loss: 1.1588 - val_accuracy: 0.6610\n",
      "Epoch 19/60\n",
      "291/291 [==============================] - 212s 730ms/step - loss: 1.1877 - accuracy: 0.6462 - val_loss: 1.1720 - val_accuracy: 0.6615\n",
      "Epoch 20/60\n",
      "291/291 [==============================] - 212s 729ms/step - loss: 1.1729 - accuracy: 0.6525 - val_loss: 1.1597 - val_accuracy: 0.6585\n",
      "Epoch 21/60\n",
      "291/291 [==============================] - 212s 729ms/step - loss: 1.1699 - accuracy: 0.6536 - val_loss: 1.1756 - val_accuracy: 0.6690\n",
      "Epoch 22/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.1441 - accuracy: 0.6568 - val_loss: 1.1689 - val_accuracy: 0.6565\n",
      "Epoch 23/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.1429 - accuracy: 0.6565 - val_loss: 1.1457 - val_accuracy: 0.6695\n",
      "Epoch 24/60\n",
      "291/291 [==============================] - 211s 727ms/step - loss: 1.1408 - accuracy: 0.6573 - val_loss: 1.2082 - val_accuracy: 0.6511\n",
      "Epoch 25/60\n",
      "291/291 [==============================] - 211s 726ms/step - loss: 1.1428 - accuracy: 0.6589 - val_loss: 1.1984 - val_accuracy: 0.6600\n",
      "Epoch 26/60\n",
      "291/291 [==============================] - 211s 727ms/step - loss: 1.1194 - accuracy: 0.6658 - val_loss: 1.1742 - val_accuracy: 0.6511\n",
      "Epoch 27/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.1074 - accuracy: 0.6649 - val_loss: 1.1669 - val_accuracy: 0.6725\n",
      "Epoch 28/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.1200 - accuracy: 0.6650 - val_loss: 1.1813 - val_accuracy: 0.6501\n",
      "Epoch 29/60\n",
      "291/291 [==============================] - 211s 727ms/step - loss: 1.0943 - accuracy: 0.6713 - val_loss: 1.1622 - val_accuracy: 0.6650\n",
      "Epoch 30/60\n",
      "291/291 [==============================] - 212s 728ms/step - loss: 1.1062 - accuracy: 0.6679 - val_loss: 1.2074 - val_accuracy: 0.6570\n",
      "Epoch 31/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.0872 - accuracy: 0.6682 - val_loss: 1.1611 - val_accuracy: 0.6595\n",
      "Epoch 32/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.1051 - accuracy: 0.6700 - val_loss: 1.1220 - val_accuracy: 0.6775\n",
      "Epoch 33/60\n",
      "291/291 [==============================] - 212s 727ms/step - loss: 1.0814 - accuracy: 0.6745 - val_loss: 1.1619 - val_accuracy: 0.6620\n",
      "Epoch 34/60\n",
      "168/291 [================>.............] - ETA: 1:22 - loss: 1.0829 - accuracy: 0.6696"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                              validation_data=validation_generator, \n",
    "                              steps_per_epoch=len(train_generator), \n",
    "                              validation_steps=len(validation_generator),\n",
    "                              epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a check point callback to save our best weights\n",
    "checkpoint = ModelCheckpoint('dog_breed_classifier_model_my.h5', \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max', \n",
    "                             save_weights_only=True)\n",
    "\n",
    "# a reducing lr callback to reduce lr when val_loss doesn't increase\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                   patience=1, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=2, min_lr=1e-7)\n",
    "\n",
    "# for early stop\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data=validation_generator, \n",
    "#                               steps_per_epoch=len(train_generator)/20, \n",
    "#                               validation_steps=len(val_gen),\n",
    "                              epochs=60,\n",
    "                              callbacks=[checkpoint, reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "path = 'C:/Users/dedsec995/Projects/tensorflow/breed_dog_only/notebook/test_1.jpg'\n",
    "img = image.load_img(path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "print(x)\n",
    "# print(x.ndim)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "# print(x.ndim)\n",
    "x = x/255.\n",
    "print(x)\n",
    "classes = model.predict(x)\n",
    "# print(fn)\n",
    "print(classes)\n",
    "print(classes[0][74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\dedsec995\\projects\\tensorflow\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_method_dog_only\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_method_dog_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
